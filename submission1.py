# -*- coding: utf-8 -*-
"""Submisieknn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AXNhAH7fqWc9FSd-ioBAfZf8m-fGLAHm
"""

import nltk
nltk.download('punkt')
nltk.download('wordnet')

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import balanced_accuracy_score
import pyphen
import numpy as np
import pandas as pd
import time
from nltk.corpus import wordnet
from dale_chall import DALE_CHALL
from nltk.tokenize import word_tokenize

dtypes = {"sentence": "string", "token": "string", "complexity": "float64"}
train = pd.read_excel('train.xlsx', dtype=dtypes, keep_default_na=False)
test = pd.read_excel('test.xlsx', dtype=dtypes, keep_default_na=False)

print('train data: ', train.shape)
print('test data: ', test.shape)

train.head()

def corpus_feature(corpus):
  if corpus =='bible':
    return [0]
  elif corpus =='biomed':
    return [1]
  else:
    return [2]

def length(word):
  return len(word)

def nr_vowels(word):
  voc='aeiou'
  c=0
  for ch in word:
    if ch in voc:
      c+=1
  return c

def is_title(word):
  return int(word.istitle())

def is_dale_chall(word):
  return int(word.lower() in DALE_CHALL) #pentru ca in dale_chall avem doar cuvinte cu litere mici

def nr_syllables(word):
  language=pyphen.Pyphen(lang='en')
  return len(language.inserted(word,'-').split('-'))

def get_all_tokens(df):
  all_words=[]
  for _,item in df.iterrows():
    #Ana are mere
    tokens=word_tokenize(item['sentence'])
    #['Ana','are','mere']
    for t in tokens:
      all_words.append(t.lower())
  return all_words

all_tokens = []
all_tokens = get_all_tokens(train)
no_all_tokens = len(get_all_tokens(train))

def word_frequency(word):
  #frecventa = numarul de aparitii in train / numarul total de cuvinte din train
  frequency = all_tokens.count(word)/no_all_tokens
  return frequency

cuvant = "JesuS"
#print(get_all_tokens(train).count(cuvant.lower()), "/",len(get_all_tokens(train)),"=")

word_frequency(cuvant.lower())

def get_word_structure_features(word):
    features = []
    features.append(nr_syllables(word))
    features.append(is_dale_chall(word))
    features.append(length(word))
    features.append(nr_vowels(word))
    features.append(is_title(word))
    features.append(word_frequency(word))
    return np.array(features)

def synsets(word):
  print(wordnet.synsets(word))
  return len(wordnet.synsets(word))

def get_wordnet_features(word):
  features = []
  features.append(synsets(word))
  return np.array(features)

def featurize(row):
    word = row['token']
    all_features = []
    all_features.extend(corpus_feature(row['corpus']))
    all_features.extend(get_word_structure_features(word))
    all_features.extend(get_wordnet_features(word))
    return np.array(all_features)

def featurize_df(df):
    nr_of_features = len(featurize(df.iloc[0]))
    nr_of_examples = len(df)
    features = np.zeros((nr_of_examples, nr_of_features))
    for index, row in df.iterrows():
        row_ftrs = featurize(row)
        features[index, :] = row_ftrs
    return features

"""Generarea de caracteristici pentru setul de train"""

X_train = featurize_df(train)
y_train = train['complex'].values

X_test = featurize_df(test)

X_test = featurize_df(test)

t0 =time.time()
for nb in range(1, 8, 2):
    model = KNeighborsClassifier(n_neighbors=nb)
    model.fit(X_train, y_train)
    preds = model.predict(X_test)

t1 = time.time()
train_time = t1-t0
print(train_time)

test_id = np.arange(7663,9001)  

np.savetxt("submisie_Kaggle_19nov.csv",np.stack((test_id,preds)).T,fmt="%d",delimiter=',',header="id,complex",comments="")