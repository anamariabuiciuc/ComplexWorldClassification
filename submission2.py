# -*- coding: utf-8 -*-
"""submisie gaussian2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nhiln36EPIHxVjJRXQkwG5SYCIxTaVDk
"""

import nltk
nltk.download('punkt')
nltk.download('wordnet')

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import balanced_accuracy_score
import pyphen
import numpy as np
import pandas as pd
import time
from nltk.corpus import wordnet
from dale_chall import DALE_CHALL
from nltk.tokenize import word_tokenize
from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import KFold, cross_val_score

dtypes = {"sentence": "string", "token": "string", "complexity": "float64"}
train = pd.read_excel('train.xlsx', dtype=dtypes, keep_default_na=False)
test = pd.read_excel('test.xlsx', dtype=dtypes, keep_default_na=False)

print('train data: ', train.shape)
print('test data: ', test.shape)

def corpus_feature(corpus):
  if corpus =='bible':
    return [0]
  elif corpus =='biomed':
    return [1]
  else:
    return [2]

def length(word):
  return len(word)

def nr_vowels(word):
  voc='aeiou'
  c=0
  for ch in word:
    if ch in voc:
      c+=1
  return c

def nr_consonants(word):
  voc='aeiou'
  c=0
  for ch in word:
    if ch not in voc:
      c+=1
  return c

def is_title(word):
  return int(word.istitle())

def is_upper(word):
  return int(word.isupper())

def is_dale_chall(word):
  return int(word.lower() in DALE_CHALL) #pentru ca in dale_chall avem doar cuvinte cu litere mici

def nr_syllables(word):
  language=pyphen.Pyphen(lang='en')
  return len(language.inserted(word,'-').split('-'))

def get_all_tokens(df):
  all_words=[]
  for _,item in df.iterrows():
    #Ana are mere
    tokens=word_tokenize(item['sentence'])
    #['Ana','are','mere']
    for t in tokens:
      all_words.append(t.lower())
  return all_words

all_tokens = []
all_tokens = get_all_tokens(train)
no_all_tokens = len(get_all_tokens(train))

def word_frequency(word):
  #frecventa = numarul de aparitii in train / numarul total de cuvinte din train
  frequency = all_tokens.count(word)/no_all_tokens
  return frequency

dtypes = {"Prefixes": "string"}
prefixes = pd.read_excel('prefixes.xlsx', dtype=dtypes, keep_default_na=False)

def has_prefixes(word):
  for i in prefixes:
    p = word.startswith(tuple(prefixes[i])) #l-am facut tuplu pentru ca doar asa se poate, altfel returneaza typeerror
    if p:
      return 1
    return 0

def get_word_structure_features(word):
    features = []
    features.append(nr_syllables(word))
    features.append(is_dale_chall(word))
    features.append(length(word))
    features.append(nr_vowels(word))
    features.append(nr_consonants(word))
    features.append(is_title(word))
    features.append(is_upper(word))
    features.append(word_frequency(word))
    features.append(has_prefixes(word))
    return np.array(features)

def synsets(word):
  print(wordnet.synsets(word))
  return len(wordnet.synsets(word))

def no_of_synonims(word):
  synonims = 0
  for ss in wordnet.synsets(word):
    synonims += len(ss.lemma_names())

  return synonims

def no_of_hypernyms(word):
  hypernyms = 0
  for ss in wordnet.synsets(word):
    hypernyms += len(ss.hypernyms())

  return hypernyms

def get_wordnet_features(word):
  features = []
  features.append(synsets(word))
  features.append(no_of_synonims(word))
  features.append(no_of_hypernyms(word))
  return np.array(features)

syn = wordnet.synsets('mother')[0]

print ("Synset name : ", syn.name())

print ("\nSynset abstract term : ", syn.hypernyms())

print ("\nSynset specific term : ",
	syn.hypernyms()[0].hyponyms())

syn.root_hypernyms()

print ("\nSynset root hypernerm : ", syn.root_hypernyms())

def featurize(row):
    word = row['token']
    all_features = []
    all_features.extend(corpus_feature(row['corpus']))
    all_features.extend(get_word_structure_features(word))
    all_features.extend(get_wordnet_features(word))
    return np.array(all_features)

def featurize_df(df):
    nr_of_features = len(featurize(df.iloc[0]))
    nr_of_examples = len(df)
    features = np.zeros((nr_of_examples, nr_of_features))
    for index, row in df.iterrows():
        row_ftrs = featurize(row)
        features[index, :] = row_ftrs
    return features

"""Generarea de caracteristici pentru setul de train"""

X_train = featurize_df(train)
y_train = train['complex'].values

X_test = featurize_df(test)

t0 = time.time()
model = GaussianNB()
cv = KFold(n_splits = 10, random_state = 1, shuffle = True)
print(cross_val_score(model, X_train, y_train, cv = cv, scoring='balanced_accuracy', n_jobs = -1).mean())
model.fit(X_train,y_train)
preds = model.predict(X_test)
t1 = time.time()
train_time = t1 - t0
print("Timpul de antrenare: ",train_time)

y_pred = cross_val_predict(model, X_train, y_train, cv=cv)
conf_matrix = confusion_matrix(y_train, y_pred) 
print(conf_matrix)

test_id = np.arange(7663,9001)  

np.savetxt("submisie_Kaggle_20nov2.csv",np.stack((test_id,preds)).T,fmt="%d",delimiter=',',header="id,complex",comments="")